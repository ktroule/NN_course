{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/NN_course/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.201441</td>\n",
       "      <td>-0.468864</td>\n",
       "      <td>-30.355617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.291041</td>\n",
       "      <td>0.777277</td>\n",
       "      <td>25.560334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.327755</td>\n",
       "      <td>0.040071</td>\n",
       "      <td>32.797526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2          y\n",
       "0 -2.201441 -0.468864 -30.355617\n",
       "1 -0.291041  0.777277  25.560334\n",
       "2 -0.327755  0.040071  32.797526"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/nfs/team292/kt22/misc/nn_course/data/linreg-data.csv',\n",
    "                   index_col = 0)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X = torch.tensor(data[['x1', 'x2']].values, dtype = torch.float)\n",
    "y = torch.tensor(data['y'].values, dtype = torch.float)\n",
    "\n",
    "# -- Shuffle observations\n",
    "idx = torch.randperm(y.size(0), dtype = torch.long)\n",
    "X, y = X[idx], y[idx]\n",
    "\n",
    "# -- Split train/test\n",
    "cutoff = int(idx.size(0) * 0.7)\n",
    "\n",
    "X_train, X_test = X[idx[:cutoff]], X[idx[cutoff:]]\n",
    "y_train, y_test = y[idx[:cutoff]], y[idx[cutoff:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "mu, sigma = X_train.mean(axis = 0), X_train.std(axis = 0)\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_test = (X_test - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.weights = torch.zeros(num_features, 1,\n",
    "                                   dtype = torch.float)\n",
    "        self.bias = torch.zeros(1, dtype = torch.float)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        net_input = torch.add(torch.mm(x, self.weights), self.bias) # equi: x @ self.weights + self.bias\n",
    "        activations = net_input # activation and input the same because this is a linear function\n",
    "        return activations.view(-1)\n",
    "    \n",
    "    def backward(self, x, yhat, y):\n",
    "        \n",
    "        # loss function is (yhat - y)Ë†2\n",
    "        # the derivatiuve is 2 * (y - yhat)\n",
    "        grad_loss_yhat = 2 * (y - yhat)\n",
    "        \n",
    "        grad_yhat_weights = -x\n",
    "        grad_yhat_bias = -1.\n",
    "        \n",
    "        # Chain rule\n",
    "        grad_loss_weights = torch.mm(grad_yhat_weights.t(),\n",
    "                                     grad_loss_yhat.view(-1 ,1)) / y.size(0)\n",
    "        \n",
    "        grad_loss_bias = torch.sum(grad_yhat_bias * grad_loss_yhat) / y.size(0)\n",
    "        \n",
    "        # return negative gradient\n",
    "        return (-1) * grad_loss_weights, (-1) * grad_loss_bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def loss(yhat, y):\n",
    "    return torch.mean((yhat - y)**2)\n",
    "\n",
    "\n",
    "def train(model, x, y, num_epochs, learning_rate=0.01):\n",
    "    cost = []\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        #### Compute outputs ####\n",
    "        yhat = model.forward(x)\n",
    "\n",
    "        #### Compute gradients ####\n",
    "        negative_grad_w, negative_grad_b = model.backward(x, yhat, y)\n",
    "\n",
    "        #### Update weights ####\n",
    "        model.weights += learning_rate * negative_grad_w\n",
    "        model.bias += learning_rate * negative_grad_b\n",
    "\n",
    "        #### Logging ####\n",
    "        # yhat = model.forward(x) # note that this is a bit wasteful here\n",
    "        curr_loss = loss(yhat, y)\n",
    "        print('Epoch: %03d' % (e+1), end=\"\")\n",
    "        print(' | MSE: %.5f' % curr_loss)\n",
    "        cost.append(curr_loss)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | MSE: 1817.39343\n",
      "Epoch: 002 | MSE: 1548.56519\n",
      "Epoch: 003 | MSE: 1330.74719\n",
      "Epoch: 004 | MSE: 1154.25964\n",
      "Epoch: 005 | MSE: 1011.26062\n",
      "Epoch: 006 | MSE: 895.39545\n",
      "Epoch: 007 | MSE: 801.51544\n",
      "Epoch: 008 | MSE: 725.44873\n",
      "Epoch: 009 | MSE: 663.81531\n",
      "Epoch: 010 | MSE: 613.87659\n",
      "Epoch: 011 | MSE: 573.41339\n",
      "Epoch: 012 | MSE: 540.62799\n",
      "Epoch: 013 | MSE: 514.06342\n",
      "Epoch: 014 | MSE: 492.53915\n",
      "Epoch: 015 | MSE: 475.09903\n",
      "Epoch: 016 | MSE: 460.96799\n",
      "Epoch: 017 | MSE: 449.51816\n",
      "Epoch: 018 | MSE: 440.24084\n",
      "Epoch: 019 | MSE: 432.72385\n",
      "Epoch: 020 | MSE: 426.63303\n",
      "Epoch: 021 | MSE: 421.69794\n",
      "Epoch: 022 | MSE: 417.69925\n",
      "Epoch: 023 | MSE: 414.45920\n",
      "Epoch: 024 | MSE: 411.83392\n",
      "Epoch: 025 | MSE: 409.70679\n",
      "Epoch: 026 | MSE: 407.98318\n",
      "Epoch: 027 | MSE: 406.58661\n",
      "Epoch: 028 | MSE: 405.45505\n",
      "Epoch: 029 | MSE: 404.53818\n",
      "Epoch: 030 | MSE: 403.79526\n",
      "Epoch: 031 | MSE: 403.19330\n",
      "Epoch: 032 | MSE: 402.70554\n",
      "Epoch: 033 | MSE: 402.31027\n",
      "Epoch: 034 | MSE: 401.99008\n",
      "Epoch: 035 | MSE: 401.73062\n",
      "Epoch: 036 | MSE: 401.52042\n",
      "Epoch: 037 | MSE: 401.35001\n",
      "Epoch: 038 | MSE: 401.21198\n",
      "Epoch: 039 | MSE: 401.10013\n",
      "Epoch: 040 | MSE: 401.00955\n",
      "Epoch: 041 | MSE: 400.93607\n",
      "Epoch: 042 | MSE: 400.87662\n",
      "Epoch: 043 | MSE: 400.82834\n",
      "Epoch: 044 | MSE: 400.78934\n",
      "Epoch: 045 | MSE: 400.75769\n",
      "Epoch: 046 | MSE: 400.73206\n",
      "Epoch: 047 | MSE: 400.71124\n",
      "Epoch: 048 | MSE: 400.69437\n",
      "Epoch: 049 | MSE: 400.68076\n",
      "Epoch: 050 | MSE: 400.66974\n",
      "Epoch: 051 | MSE: 400.66071\n",
      "Epoch: 052 | MSE: 400.65347\n",
      "Epoch: 053 | MSE: 400.64758\n",
      "Epoch: 054 | MSE: 400.64282\n",
      "Epoch: 055 | MSE: 400.63898\n",
      "Epoch: 056 | MSE: 400.63583\n",
      "Epoch: 057 | MSE: 400.63330\n",
      "Epoch: 058 | MSE: 400.63129\n",
      "Epoch: 059 | MSE: 400.62964\n",
      "Epoch: 060 | MSE: 400.62827\n",
      "Epoch: 061 | MSE: 400.62714\n",
      "Epoch: 062 | MSE: 400.62625\n",
      "Epoch: 063 | MSE: 400.62558\n",
      "Epoch: 064 | MSE: 400.62500\n",
      "Epoch: 065 | MSE: 400.62451\n",
      "Epoch: 066 | MSE: 400.62415\n",
      "Epoch: 067 | MSE: 400.62381\n",
      "Epoch: 068 | MSE: 400.62357\n",
      "Epoch: 069 | MSE: 400.62338\n",
      "Epoch: 070 | MSE: 400.62317\n",
      "Epoch: 071 | MSE: 400.62308\n",
      "Epoch: 072 | MSE: 400.62296\n",
      "Epoch: 073 | MSE: 400.62286\n",
      "Epoch: 074 | MSE: 400.62280\n",
      "Epoch: 075 | MSE: 400.62271\n",
      "Epoch: 076 | MSE: 400.62271\n",
      "Epoch: 077 | MSE: 400.62268\n",
      "Epoch: 078 | MSE: 400.62262\n",
      "Epoch: 079 | MSE: 400.62259\n",
      "Epoch: 080 | MSE: 400.62256\n",
      "Epoch: 081 | MSE: 400.62256\n",
      "Epoch: 082 | MSE: 400.62256\n",
      "Epoch: 083 | MSE: 400.62250\n",
      "Epoch: 084 | MSE: 400.62250\n",
      "Epoch: 085 | MSE: 400.62250\n",
      "Epoch: 086 | MSE: 400.62250\n",
      "Epoch: 087 | MSE: 400.62250\n",
      "Epoch: 088 | MSE: 400.62250\n",
      "Epoch: 089 | MSE: 400.62250\n",
      "Epoch: 090 | MSE: 400.62250\n",
      "Epoch: 091 | MSE: 400.62250\n",
      "Epoch: 092 | MSE: 400.62250\n",
      "Epoch: 093 | MSE: 400.62250\n",
      "Epoch: 094 | MSE: 400.62247\n",
      "Epoch: 095 | MSE: 400.62250\n",
      "Epoch: 096 | MSE: 400.62247\n",
      "Epoch: 097 | MSE: 400.62250\n",
      "Epoch: 098 | MSE: 400.62247\n",
      "Epoch: 099 | MSE: 400.62250\n",
      "Epoch: 100 | MSE: 400.62250\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(num_features = X_train.size(1))\n",
    "\n",
    "cost = train(model, \n",
    "             X_train, \n",
    "             y_train, \n",
    "             num_epochs = 100, \n",
    "             learning_rate = 0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN_course",
   "language": "python",
   "name": "nn_course"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
