{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADALINE automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.201441</td>\n",
       "      <td>-0.468864</td>\n",
       "      <td>-30.355617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.291041</td>\n",
       "      <td>0.777277</td>\n",
       "      <td>25.560334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.327755</td>\n",
       "      <td>0.040071</td>\n",
       "      <td>32.797526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2          y\n",
       "0 -2.201441 -0.468864 -30.355617\n",
       "1 -0.291041  0.777277  25.560334\n",
       "2 -0.327755  0.040071  32.797526"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/nfs/team292/kt22/misc/nn_course/data/linreg-data.csv',\n",
    "                   index_col = 0)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(data[['x1', 'x2']].values, dtype = torch.float)\n",
    "y = torch.tensor(data['y'].values, dtype = torch.float)\n",
    "\n",
    "# -- Shuffle observations\n",
    "idx = torch.randperm(y.size(0), dtype = torch.long)\n",
    "X, y = X[idx], y[idx]\n",
    "\n",
    "# -- Split train/test\n",
    "cutoff = int(idx.size(0) * 0.7)\n",
    "\n",
    "X_train, X_test = X[idx[:cutoff]], X[idx[cutoff:]]\n",
    "y_train, y_test = y[idx[:cutoff]], y[idx[cutoff:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = X_train.mean(axis = 0), X_train.std(axis = 0)\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_test = (X_test - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class ADALINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(yhat, y):\n",
    "    return torch.mean((yhat - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline3(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features):\n",
    "        super(Adaline3, self).__init__()\n",
    "        self.linear = torch.nn.Linear(in_features = num_features,\n",
    "                                      out_features = 1)\n",
    "        \n",
    "        # change random weights to zero\n",
    "        # (don't do this for multi-layer nets!)\n",
    "        self.linear.weight.detach().zero_()\n",
    "        self.linear.bias.detach().zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        netinputs = self.linear(x)\n",
    "        activations = netinputs\n",
    "        return activations.view(-1)\n",
    "\n",
    "    \n",
    "####################################################\n",
    "##### Training and evaluation wrappers\n",
    "###################################################\n",
    "\n",
    "\n",
    "def train(model, x, y, num_epochs, learning_rate = 0.01, seed = 123, minibatch_size = 10):\n",
    "    cost = []\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr = learning_rate)\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        \n",
    "        #### Shuffle epoch\n",
    "        shuffle_idx = torch.randperm(y.size(0),\n",
    "                                     dtype = torch.long)\n",
    "        minibatches = torch.split(shuffle_idx, minibatch_size)\n",
    "        \n",
    "        for minibatch_idx in minibatches:\n",
    "\n",
    "            #### Compute outputs ####\n",
    "            yhat = model.forward(x[minibatch_idx])\n",
    "            \n",
    "            # you could also use our \"manual\" loss_func\n",
    "            loss = F.mse_loss(yhat, y[minibatch_idx])\n",
    "            \n",
    "            #### Reset gradients from previous iteration ####\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #### Compute gradients ####\n",
    "            loss.backward()\n",
    "            \n",
    "            #### Update weights ####\n",
    "            optimizer.step()\n",
    "\n",
    "        #### Logging ####\n",
    "        with torch.no_grad():\n",
    "            # context manager to\n",
    "            # avoid building graph during \"inference\"\n",
    "            # to save memory\n",
    "            yhat = model.forward(x)\n",
    "            curr_loss = loss_func(yhat, y)\n",
    "            print('Epoch: %03d' % (e+1), end=\"\")\n",
    "            print(' | MSE: %.5f' % curr_loss)\n",
    "            cost.append(curr_loss)\n",
    "\n",
    "    return cost, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | MSE: 478.79483\n",
      "Epoch: 002 | MSE: 389.34915\n",
      "Epoch: 003 | MSE: 384.71274\n",
      "Epoch: 004 | MSE: 384.27646\n",
      "Epoch: 005 | MSE: 384.39951\n",
      "Epoch: 006 | MSE: 384.31329\n",
      "Epoch: 007 | MSE: 384.53223\n",
      "Epoch: 008 | MSE: 384.33130\n",
      "Epoch: 009 | MSE: 384.23593\n",
      "Epoch: 010 | MSE: 384.29434\n",
      "Epoch: 011 | MSE: 384.61581\n",
      "Epoch: 012 | MSE: 384.42798\n",
      "Epoch: 013 | MSE: 384.36292\n",
      "Epoch: 014 | MSE: 384.35608\n",
      "Epoch: 015 | MSE: 384.24762\n",
      "Epoch: 016 | MSE: 384.44861\n",
      "Epoch: 017 | MSE: 384.31253\n",
      "Epoch: 018 | MSE: 384.29813\n",
      "Epoch: 019 | MSE: 384.42749\n",
      "Epoch: 020 | MSE: 384.29214\n"
     ]
    }
   ],
   "source": [
    "model = Adaline3(num_features = X_train.size(1))\n",
    "\n",
    "cost, optimizer = train(model,\n",
    "                        X_train,\n",
    "                        y_train.float(),\n",
    "                        num_epochs=20,\n",
    "                        learning_rate=0.01,\n",
    "                        seed=123,\n",
    "                        minibatch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SGD.step of SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    foreach: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN_course",
   "language": "python",
   "name": "nn_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
